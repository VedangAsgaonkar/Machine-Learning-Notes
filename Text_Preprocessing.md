We generally follow these steps for preprocessing text data:
* **Tokenisation** : We split the text into individual sentences and then into individual words. We may also remove small words, numbers, unknown characters, punctuations, URLs and change everything to small case, depending on our application
* **Indexing** : Assign an index to each unique word. This is its one hot encoding vector. We generally start indexing from 2 and we reserve 0 for unknown words(words absent in our embedding layer) and 1 for padding
* **Constant Length Sequences** : We may truncate long sentences into short ones. Similarly for short sentences, we may pad the padding character. Pre-padding i.e padding before the sentence starts is preferred as RNNs and LSTMs may forget the actual sentence in case of Post-padding due to short memory. Very often, the data is divided into batches, and we try to keep sentences of similar length in one batch, allowing for better training
