Word embeddings are a way to capture similarity between words in their vector
representations. [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa).
There are two main algorithms used to make word embeddings :
* [Word2Vec â€” Skip-gram and CBOW](https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314)
* [Negative Sampling and GloVe](https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68)
