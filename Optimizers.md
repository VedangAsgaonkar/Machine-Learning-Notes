### Gradient Descent
[Learning Parameters, Part 1: Gradient Descent](https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb)

### Momentum
This optimizer helps to overcome regions of low slope [Learning Parameters, Part 2: Momentum-Based & Nesterov Accelerated Gradient Descent
](https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12)

### Stochastic Gradient Descent
Stochastic Gradient Descent is different from Gradient Descent in the fact that Gradient Descent takes the whole data and finds the loss, while SGD just takes a random sample subset of the data. [Learning Parameters, Part 3: Stochastic & Mini-Batch Gradient Descent](https://towardsdatascience.com/learning-parameters-part-3-ee8558f65dd7)

### Choosing the learning rate
We can judge how good our learning rate is by seeing the graph of losses converging. We generally try powers of 10 like 0.1, 0.01, 0.001
