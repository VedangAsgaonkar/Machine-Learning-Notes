### Stochastic Gradient Descent
Stochastic Gradient Descent is different from Gradient Descent in the fact that Gradient Descent takes the whole data and finds the loss, while SGD just takes a random sample subset of the data. [Learning Parameters, Part 1: Gradient Descent](https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb)

### Choosing the learning rate
We can judge how good our learning rate is by seeing the graph of losses converging. We generally try powers of 10 like 0.1, 0.01, 0.001
