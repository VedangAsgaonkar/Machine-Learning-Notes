### Stochastic Gradient Descent
Stochastic Gradient Descent is different from Gradient Descent in the fact that Gradient Descent takes the whole data and finds the loss, while SGD just takes a random sample subset of the data

### Choosing the learning rate
We can judge how good our learning rate is by seeing the graph of losses converging. We generally try powers of 10 like 0.1, 0.01, 0.001
